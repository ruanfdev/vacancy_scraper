name: Vacancy Scraper

on:
  schedule:
    - cron: '0 8 * * *'  # Run daily at 8:00 AM (Adjust as needed)
  workflow_dispatch:     # Keep workflow_dispatch for manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        node-version: [20.x]

    steps:
      - uses: actions/checkout@v4
      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
      - name: Install dependencies
        run: npm install puppeteer fs 

      # Manually download and setup Chromium
      - name: Download and install Chromium
        env:
          PUPPETEER_SKIP_CHROMIUM_DOWNLOAD: true # Skip Puppeteer's automatic download
        run: |
          # Get the latest stable Chromium version number
          CHROMIUM_VERSION=$(curl -s https://omahaproxy.appspot.com/ | grep -Po '(?<=chromium_rel":")[^"]*')
          echo "Installing Chromium version: $CHROMIUM_VERSION"

          # Install Chromium using apt-get
          sudo apt-get update
          sudo apt-get install -y chromium-$CHROMIUM_VERSION

      - name: Run the scraper
        env:
          PUPPETEER_EXECUTABLE_PATH: /usr/bin/chromium-browser # Set the path to Chromium
        run: node vacancy_scraper.js

      - uses: actions/upload-artifact@v4
        with:
          name: vacancies
          path: vacancies.csv

      - uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: error-screenshot
          path: error_screenshot.png