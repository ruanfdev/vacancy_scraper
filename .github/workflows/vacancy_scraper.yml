name: Vacancy Scraper

on:
  schedule:
    - cron: '0 8 * * *'  # Run daily at 8:00 AM (Adjust as needed)
  workflow_dispatch:     # Keep workflow_dispatch for manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        node-version: [20.x]

    steps:
      - uses: actions/checkout@v4
      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
      - name: Install dependencies
        run: npm install puppeteer fs

      # Manually download and setup Chromium
      - name: Download and install Chromium
        env:
          PUPPETEER_SKIP_CHROMIUM_DOWNLOAD: true # Skip Puppeteer's automatic download
        run: |
          # Ensure apt-get is up-to-date and install necessary packages
          sudo apt-get update && sudo apt-get install -y wget gnupg2

          # Add Google's signing key for package verification
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -

          # Add Google Chrome to the package sources
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'

          # Update the package list again
          sudo apt-get update

          # Install the latest stable Chromium
          sudo apt-get install -y google-chrome-stable

          # Verify Chromium installation
          google-chrome --version || (echo "Chromium installation failed" && exit 1)

      # Run the scraper
      - name: Run the scraper
        env:
          PUPPETEER_EXECUTABLE_PATH: /usr/bin/google-chrome # Set the path to Chromium
        run: node vacancy_scraper.mjs

      - uses: actions/upload-artifact@v4
        with:
          name: vacancies
          path: vacancies.csv

      - uses: actions/upload-artifact@v3
        if: failure()
        with:
          name: error-screenshot
          path: error_screenshot.png

